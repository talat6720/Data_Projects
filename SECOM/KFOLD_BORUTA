import importlib.util
print('#########################################################################')
print('                        Checking and Installing Packages                 ')                       
print('#########################################################################')
packages = ['sklearn','pandas','zipfile','scipy','boruta','sklearn.ensemble','scipy.stats','sklearn.metrics','matplotlib','sklearn.linear_model', 'sklearn.model_selection', 'imblearn','sklearn.preprocessing','sklearn.impute']

 

for package in packages:
    try:
        importlib.import_module(package)
        print(f"{package} is already installed.")
    except ImportError:
        print(f"{package} is not installed. Installing...")
        !pip install {package}

        
import pandas as pd
import os
import zipfile
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedShuffleSplit
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
import urllib.request
import numpy as np
from scipy import stats
from scipy.stats import zscore
import seaborn as sns
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from boruta import BorutaPy
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
##############################################################################
#
#############################################################################
def find_same_val_cols_count(df):
    same_value_cols = [col for col in df.columns if df[col].nunique() == 1]
    num_same_value_cols = len(same_value_cols)
    print(f"Columns names with only one unique value: {same_value_cols}")
    print(f"Number of columns count with only one unique value: {num_same_value_cols}")
    return same_value_cols, num_same_value_cols


def calculate_missing_percentages_per_column(df):
    # Calculate the percentage of missing values for each column
    missing_percentages = df.isna().mean() * 100

    # Create a new DataFrame to store the missing percentages
    missing_df = pd.DataFrame({'Feature': missing_percentages.index, 'Missing Percentage': missing_percentages.values})
    
    # Missing values in entire training Dataset
    total_missing = df.isnull().sum().sum()
    total_cells = df.shape[0] * df.shape[1]
    missing_percentage_train = (total_missing / total_cells) * 100
    print("Percentage of missing values in the entire dataset: {:.2f}%".format(missing_percentage_train))

    return missing_df


def plot_missing_percentage_histogram(df, missing_threshold):
    # Calculate the percentage of missing values for each column
    missing_percentages = df.isna().mean() * 100

    # Create a new DataFrame to store the missing percentages
    missing_df = pd.DataFrame({'Feature': missing_percentages.index, 'Missing Percentage': missing_percentages.values})

    # Plot a histogram of missing values
    fig, ax = plt.subplots(figsize=(10, 8))
    ax.hist(missing_df['Missing Percentage'], bins=20, color='Green', edgecolor='black')
    ax.set_xlabel('Percentage of missing values')
    ax.set_ylabel('Number of features')

    # Add a vertical line at the specified percentage threshold
    ax.axvline(x=missing_threshold, color='red')

    # Add a title to the plot
    ax.set_title(f'Histogram of Missing Values (Threshold: {missing_threshold}%)')

    # Show the plot
    plt.show()


def calculate_target_column_distribution(dataframe, column_name):
    # Calculate the distribution of the column
    distribution = dataframe[column_name].value_counts(normalize=True) * 100

    # Print the distribution
    print(distribution)


'''def calculate_outliers(dataframe):
    # Creating a copy of a dataframe
    merged_df_copy = dataframe.copy()
    # drop the non-numeric columns
    merged_df_copy = merged_df_copy.drop(['timestamp', 'date', 'time'], axis=1)
    # Calculate the z-scores for each variable
    z_scores = (merged_df_copy - merged_df_copy.mean()) / merged_df_copy.std()

    # Find the outliers for each variable
    outliers = z_scores[(z_scores > 3) | (z_scores < -3)]

    # Count variables with outliers
    outliers_count = (outliers != 0).sum()
    print("Number of variables with outliers:", len(outliers_count[outliers_count != 0]))

    # Count the number of outliers for each feature
    outlier_counts = outliers.notnull().sum(axis=0)

    # Print the number of outliers for each feature
    print("Outlier counts for each feature:", outlier_counts)
    # print(outlier_counts)

    # Count the total number of outliers
    total_outliers = outlier_counts.sum()
    print("Total number of outliers:", total_outliers)

    # Save outlier counts to a new dataframe
    outlier_df = pd.DataFrame({"Feature": outlier_counts.index, "Outlier Count": outlier_counts.values})

    return outlier_df'''


def plot_variance_histogram(df):
    # Select only the numeric columns
    numeric_df = df.select_dtypes(include='number')
    # Calculate the variance of every column

    variances = numeric_df.var()
    #print(variances)

    # Plot the histogram of variances
    variance_df = variances.to_frame(name='variance')
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.hist(variance_df, bins=50, alpha=0.5, color='Green', edgecolor='black')
    ax.set_xlabel('Variance')
    ax.set_ylabel('Frequency')
    ax.set_title('Histogram of Variances')
    plt.show()


def plot_pass_fail_results_over_time(dataframe):
    # Convert the 'timestamp' column to a datetime object
    dataframe['timestamp'] = pd.to_datetime(dataframe['timestamp'], dayfirst=True)

    # Create a new column 'Result' based on 'Pass/Fail' column
    dataframe['Result'] = dataframe['pass/fail'].apply(lambda x: 'Fail' if x == 1 else 'Pass')

    # Group the data by date and 'Result'
    dataframe = dataframe[['timestamp', 'Result']].copy()
    dataframe.set_index('timestamp', inplace=True)
    result_by_date = dataframe.groupby([pd.Grouper(freq='D'), 'Result'])['Result'].count().unstack()

    # Fill in any missing values with 0
    result_by_date = result_by_date.fillna(0)

    # Rename the columns to 'Pass' and 'Fail'
    result_by_date.columns = ['Fail', 'Pass']

    # Create a line chart showing the number of passes and fails over time
    plt.plot(result_by_date.index, result_by_date['Pass'], label='Pass')
    plt.plot(result_by_date.index, result_by_date['Fail'], label='Fail')
    plt.xlabel('Date')
    plt.ylabel('Count of Pass/Fail')
    plt.title('Pass/Fail Results over Time')
    plt.legend()
    plt.show()


def find_outlier(dataframe, threshold=3):
    # Replace non-numeric values with NaN
    dataframe = dataframe.apply(pd.to_numeric, errors='coerce')

    # Create a copy of the DataFrame
    dataframe_copy = dataframe.copy()

    # Find the absolute z-scores of the data
    z_scores = np.abs(stats.zscore(dataframe_copy))

    # Select the outliers
    outliers = dataframe_copy[(z_scores > threshold).any(axis=1)]

    # Replace the outliers with NaN
    dataframe_copy[(z_scores > threshold)] = np.nan

    # Count the number of outliers for each variable
    outlier_counts = pd.Series((z_scores > threshold).sum(axis=0), index=dataframe.columns)

    # Print the outlier counts in descending order
    print(outlier_counts.sort_values(ascending=False))

    return dataframe_copy


from sklearn.model_selection import StratifiedShuffleSplit


def split_data_by_stratification(dataframe, target_variable, test_size=0.2, random_state=42):
    # Define target variable and stratum
    stratum = dataframe[target_variable]

    # Define Stratified Shuffle Split with specified split ratio
    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)

    # Split dataset into training and testing sets
    for train_index, test_index in split.split(dataframe, stratum):
        train_set = dataframe.loc[train_index]
        test_set = dataframe.loc[test_index]

    # Print the sizes of the training and testing sets
    print("Training set size:", len(train_set))
    print("Testing set size:", len(test_set))

    return train_set, test_set

def plot_correlation_heatmap(df):

    #df['timestamp'] = pd.to_numeric(df['timestamp'])
    # Compute the correlation matrix
    corr = df.corr()

    # Generate a heatmap
    sns.heatmap(corr, cmap='PuOr')

    # Show the plot
    plt.show()
    
def plot_outlier_count_by_variable(dataframe, threshold=3):
    # Select the numerical columns to analyze for outliers
    num_cols = dataframe.select_dtypes(include=[np.number]).columns.tolist()

    # Calculate the Z-score for each value in each column
    z_scores = dataframe[num_cols].apply(zscore)

    # Create a boolean DataFrame to mark outliers
    is_outlier = (z_scores > threshold) | (z_scores < -threshold)

    # Count the number of outliers in each column
    outlier_count = is_outlier.sum()

    # Filter the outlier count to only include variables with outliers
    outlier_count = outlier_count[outlier_count > 0]

    # Sort the outlier count in descending order
    outlier_count = outlier_count.sort_values(ascending=False)

    # Create a bar chart of the outlier count
    fig = plt.figure(figsize=(14, 5), facecolor='w')

    # Define the color palette with shades of green
    color_palette = plt.get_cmap('Greens')(np.linspace(0.4, 1, len(outlier_count)))

    outlier_count.plot.bar(color=color_palette)

    plt.xticks(rotation=45)
    plt.xlabel('Variable')
    plt.ylabel('Number of outliers')
    plt.title('Count of outliers by variable')
    plt.show()


def drop_columns_with_missing_values(data, threshold):
    # Calculate the threshold number of non-null values required to keep the column
    threshold_value = len(data) * threshold

    # Drop columns with missing values more than the threshold
    modified_data = data.dropna(thresh=threshold_value, axis=1)

    # Calculate the number of columns deleted
    num_columns_deleted = data.shape[1] - modified_data.shape[1]

    # Print the new shape of the dataset
    print("New shape of the dataset: ", modified_data.shape)
    print("Number of columns deleted: ", num_columns_deleted)

    return modified_data

def remove_same_val_cols(df):
    same_value_cols = [col for col in df.columns if df[col].nunique() == 1]
    num_same_value_cols = len(same_value_cols)
    print(f"Columns names with only one unique value: {same_value_cols}")
    print(f"Number of columns count with only one unique value: {num_same_value_cols}")
    df.drop(same_value_cols, axis=1, inplace=True)
    print("Columns with only one unique value have been removed from the dataframe.")

    
def filter_test_set(df1, df2):
    df2_filtered = df2.copy()
    
    # Get the common features/columns in both datasets
    common_features = list(set(df1.columns) & set(df2.columns))

    # Delete the features/columns in df2 not present in df1
    df2_filtered = df2_filtered.drop(columns=[col for col in df2.columns if col not in common_features])

    return df2_filtered


def scale_data(train_data, test_data):
    # Separate the features and the target variable for training data
    X_train = train_data.iloc[:, :-1].values
    y_train = train_data.iloc[:, -1].values
    
    # Separate the features and the target variable for testing data
    X_test = test_data.iloc[:, :-1].values
    y_test = test_data.iloc[:, -1].values
    
    # Initialize the scaler
    scaler = StandardScaler()
    
    # Scale the features for training data
    X_train_scaled = scaler.fit_transform(X_train)
    
    # Scale the features for testing data using the same scaler object from training data
    X_test_scaled = scaler.transform(X_test)
    
    # Create new dataframes with the scaled features and original target variable
    scaled_train_data = pd.DataFrame(X_train_scaled, columns=train_data.columns[:-1])
    scaled_train_data[train_data.columns[-1]] = y_train
    
    scaled_test_data = pd.DataFrame(X_test_scaled, columns=test_data.columns[:-1])
    scaled_test_data[test_data.columns[-1]] = y_test
    
    return scaled_train_data, scaled_test_data


def knn_imputation(df, n_neighbors=5):
    # Calculate the number of missing values before imputation
    num_missing_before = df.isnull().sum().sum()
    # Create a KNNImputer object with the desired number of neighbors
    imputer = KNNImputer(n_neighbors=n_neighbors)

    # Perform KNN imputation on the dataframe
    imputed_data = imputer.fit_transform(df)

    # Convert the imputed numpy array back to a dataframe
    imputed_df = pd.DataFrame(imputed_data, columns=df.columns)
    # Calculate the number of imputed values
    num_imputed = df.isnull().sum().sum() - imputed_df.isnull().sum().sum()
    print("Number of missing values before imputation:", num_missing_before)

    print("Number of imputed values:", num_imputed)
    # Calculate the current number of missing values
    num_missing_current = imputed_df.isnull().sum().sum()
    print("Number of missing values after imputation:", num_missing_current)
    return imputed_df


def apply_smote(X_train, y_train, sampling_strategy='auto', random_state=42):
    # Create SMOTE object
    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state)

    # Apply SMOTE to the training data
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    return X_train_resampled, y_train_resampled



def perform_kfold_feature_selection(X_train, y_train, X_test, y_test, n_splits=2):
    
    # Convert X_train, X_test to pandas DataFrames
    X_train = pd.DataFrame(X_train)
    X_test = pd.DataFrame(X_test)
    y_train = pd.DataFrame(y_train, columns=["pass/fail"])
    y_test = pd.DataFrame(y_test, columns=["pass/fail"])

    # Initialize lists to store the results for each fold
    fold_accuracies = []
    fold_cm = []

    # Initialize Boruta feature selector with Random Forest estimator
    boruta_selector = BorutaPy(estimator=RandomForestClassifier(n_estimators=100, max_depth=5), n_estimators= 100,max_iter=50, verbose=2)

    # Perform k-fold cross-validation
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    for train_index, val_index in skf.split(X_train, y_train):
        # Split the data into training and validation sets for the current fold
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Perform feature selection using Boruta
        boruta_selector.fit(X_train_fold.values, y_train_fold.values.ravel())

        # Get the selected features from Boruta
        selected_features = X_train.columns[boruta_selector.support_]

        # Subset the training and validation data with the selected features
        X_train_selected = X_train_fold[selected_features]
        X_val_selected = X_val_fold[selected_features]

        # Create the logistic regression model
        logistic_model = LogisticRegression(max_iter=1000)

        # Train the model
        logistic_model.fit(X_train_selected, y_train_fold.values.ravel())

        # Make predictions on the validation set
        y_val_pred = logistic_model.predict(X_val_selected)

        # Calculate the accuracy of the model on the validation set
        accuracy = accuracy_score(y_val_fold, y_val_pred)
        fold_accuracies.append(accuracy)

        # Generate confusion matrix for the validation set
        cm = confusion_matrix(y_val_fold, y_val_pred)
        fold_cm.append(cm)

    # Train the final model using all training data and selected features
    boruta_selector.fit(X_train.values, y_train.values.ravel())
    selected_features = X_train.columns[boruta_selector.support_]
    X_train_selected = X_train[selected_features]
    X_test_selected = X_test[selected_features]
    logistic_model = LogisticRegression(max_iter=1000)
    logistic_model.fit(X_train_selected, y_train.values.ravel())

    # Make predictions on the test set
    y_test_pred = logistic_model.predict(X_test_selected)

    # Calculate the accuracy of the final model on the test set
    test_accuracy = accuracy_score(y_test, y_test_pred)

    # Generate confusion matrix for the test set
    test_cm = confusion_matrix(y_test, y_test_pred)

    # Return the results
    return fold_accuracies, fold_cm, test_accuracy, test_cm
#########################################################################

#########################################################################
# URL of the zip file
url = 'https://archive.ics.uci.edu/static/public/179/secom.zip'
# Download the zip file
urllib.request.urlretrieve(url, 'secom.zip')

# Extract the contents of the zip file
with zipfile.ZipFile('secom.zip', 'r') as zip_ref:
    # Assuming the first file in the zip is the data file
    secom_file = zip_ref.namelist()[0]
    # Assuming the third file in the zip is the label file
    label_file = zip_ref.namelist()[2]

    # Read the data files into DataFrames
    secom_data = pd.read_csv(zip_ref.open(secom_file), sep=' ', header=None)
    secom_labels = pd.read_csv(zip_ref.open(label_file), sep=' ', header=None)

#secom_data = pd.read_csv("secom_data.txt", delimiter="\s+", header=None)
#secom_labels = pd.read_csv("secom_labels.txt", delimiter="\s+", header=None)

# merge the dataframes based on a common column
merged_df = pd.concat([secom_data, secom_labels], axis=1)

# provide automatic column names to data
column_names = ["feature_" + str(i) for i in range(1, merged_df.shape[1] + 1)]

# assign column names
merged_df.columns = column_names
# set the column names for the dataframe
merged_df = merged_df.rename(columns={merged_df.columns[-2]: 'pass/fail'})
merged_df = merged_df.rename(columns={merged_df.columns[-1]: 'timestamp'})
print('###################################################################################')
print('                        Glimpse of Data                                            ')
print('###################################################################################')

print(merged_df.head())

# create separate date and time columns
merged_df['timestamp'] = pd.to_datetime(merged_df['timestamp'])
merged_df['date'] = merged_df['timestamp'].dt.date
merged_df['time'] = merged_df['timestamp'].dt.time
print(merged_df.head())


#######################################################################
#              Calling function starts from here
#######################################################################

print('###################################################################################')
print('               Missing values percentage per column and in entire Datset   ')
print('###################################################################################')
missing = calculate_missing_percentages_per_column(merged_df)
print(missing)
print('###################################################################################')
print('                           Features having constant values                         ')
print('###################################################################################')
constant_value = find_same_val_cols_count(merged_df)
print('###################################################################################')
print('                              Histogram of missing values                          ')
print('###################################################################################')
plot_missing_percentage_histogram(merged_df, 60)
print('###################################################################################')
print('                   Target column Distribution for entire Data                      ')
print('###################################################################################')
calculate_target_column_distribution(merged_df, 'pass/fail')
print('###################################################################################')
print('                                  Histogram of Varience                            ')
print('###################################################################################')
plot_variance_histogram(merged_df)
#calculate_outliers(merged_df)
print('###################################################################################')
print('                            Pass and fail result over time                         ')
print('###################################################################################')
plot_pass_fail_results_over_time(merged_df)
print('###################################################################################')
print('                                    Outliers in Features                           ')
print('###################################################################################')
outlier = find_outlier(merged_df, threshold=3)
train_set, test_set = split_data_by_stratification(merged_df, 'pass/fail', test_size=0.2, random_state=42)
print('###################################################################################')
print('                      Target column distribution for Training Data                 ')
print('###################################################################################')
train_freq_distribution = calculate_target_column_distribution(train_set,'pass/fail')
print('###################################################################################')
print('                              Outliers in Training Dataset                         ')
print('###################################################################################')
plot_outlier_count_by_variable(train_set)
print('###################################################################################')
print('                                Correlation Heatmap                                ')
print('###################################################################################')
plot_correlation_heatmap(train_set)
print(train_freq_distribution)
#print(constant_value)
train_set = train_set.drop(columns=train_set.columns[-4:])
train_set.head()
train_set_copy = train_set.copy()
test_set_copy = test_set.copy()
print('###################################################################################')
print('                        Deleted count of features above threshold                  ')
print('###################################################################################')
train_set_copy = drop_columns_with_missing_values(train_set_copy, 0.4)
print('###################################################################################')
print('                 Count of Features having same values and deleted features         ')
print('###################################################################################')
remove_same_val_cols(train_set_copy)
test_set_copy = filter_test_set(train_set_copy, test_set_copy)# for making the train and test datasets in same dimension
scaled_train_set, scaled_test_set = scale_data(train_set_copy, test_set_copy)#performing scaling on both the datasets
print('###################################################################################')
print('                               Imputation in Training Dataset                      ')
print('###################################################################################')
imputed_train_df_knn = knn_imputation(scaled_train_set, n_neighbors=3)
print('###################################################################################')
print('                            Imputation In testing Dataset                          ')
print('###################################################################################')
imputed_test_df_knn = knn_imputation(scaled_test_set, n_neighbors=3)
# Separate the features and the target variable for both the dataset
X_train = imputed_train_df_knn.iloc[:, :-1].values
y_train = imputed_train_df_knn.iloc[:, -1].values
X_test = imputed_test_df_knn.iloc[:, :-1].values
y_test = imputed_test_df_knn.iloc[:, -1].values
# Apply SMOTE to the training data
X_train_resampled, y_train_resampled = apply_smote(X_train, y_train)
# The testing data remains unchanged
X_test_resampled, y_test_resampled = X_test, y_test
# Call the logistic_regression function
# Assuming you have your training and testing data arrays defined
X_train = imputed_train_df_knn.iloc[:, :-1].values
y_train = imputed_train_df_knn.iloc[:, -1].values
X_test = imputed_test_df_knn.iloc[:, :-1].values
y_test = imputed_test_df_knn.iloc[:, -1].values
fold_accuracies, fold_cm, test_accuracy, test_cm = perform_kfold_feature_selection(X_train, y_train, X_test, y_test)

print('###################################################################################')
print('                          Fold Accuracy and Confusion Matrix                       ')
print('###################################################################################')
# Print the results
print("Fold Accuracies:", fold_accuracies)
print("\nConfusion Matrix:")
print(fold_cm)
print('###################################################################################')
print('                          Test Accuracy and Confusion Matrix                       ')
print('###################################################################################')

print("Test Accuracy:", test_accuracy)
print("Test Confusion Matrix:")
print(test_cm)
print('Roadmap till now')
print('Scaling -> KNN Imputation -> SMOTE -> Boruta -> Logistic Regression -> KFold ')
